# AI Agent Configuration File
# This file contains all configuration settings for the AI Agent microservice

# Application Settings
app:
  name: "AI Agent Service"
  version: "1.0.0"
  host: "0.0.0.0"
  port: 8001
  debug: false
  log_level: "INFO"

# LLM Configuration
llm:
  model: "gemini-2.5-flash"
  temperature: 0.7
  max_tokens: 2048
  timeout: 30

# Embedding Configuration
embedding:
  # Using HuggingFace Inference API (no GPU needed)
  provider: "huggingface"
  model: "sentence-transformers/all-MiniLM-L6-v2"  # Fast, efficient, 384 dimensions
  # Alternative models:
  # "sentence-transformers/all-mpnet-base-v2"  # Better quality, 768 dimensions, slower
  # "BAAI/bge-small-en-v1.5"  # Good balance, 384 dimensions
  # "intfloat/e5-base-v2"  # High quality, 768 dimensions
  timeout: 30
  max_retries: 3
  batch_size: 32  # Process up to 32 texts in one API call
  cache_embeddings: true
  embedding_cache_ttl: 86400  # Cache embeddings for 24 hours

# MCP Server Configuration
mcp:
  server_url: "http://mcp-knowledge-server:8000/knowledgeMCP/"
  timeout: 30
  retry_attempts: 3
  connection_retry_delay: 2  # seconds between retries

# Friend Service Configuration
friend_service:
  base_url: "http://friend:8085"
  timeout: 30

# Database Configurations
databases:
  # MongoDB Configuration (from docker-compose: generatedData service)
  mongodb:
    url: "mongodb://mongo_user:example@generatedData:27017"
    database: "generated_db"
    connection_timeout: 5000
    server_selection_timeout: 5000
    max_pool_size: 100
    min_pool_size: 10
  
  # Redis Configuration (from docker-compose: redis service)
  redis:
    url: "redis://redisCache:6379"
    database: 0
    connection_timeout: 5
    retry_on_timeout: true
    max_connections: 20

# Cache Configuration
cache:
  default_ttl: 3600  # 1 hour in seconds
  friend_summary_ttl: 7200  # 2 hours for friend summaries
  tool_cache_ttl: 1800  # 30 minutes for tool results

# Knowledge Service Configuration
knowledge:
  max_knowledge_items_per_request: 30
  summarization_batch_size: 10
  enable_caching: true

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file:
    enabled: true
    path: "logs/ai_agent.log"
    max_size: "10MB"
    backup_count: 5
  console:
    enabled: true

# Security Configuration
security:
  cors:
    allow_origins: ["*"]
    allow_credentials: true
    allow_methods: ["*"]
    allow_headers: ["*"]

# Development/Production Overrides
development:
  debug: true
  log_level: "DEBUG"
  
production:
  debug: false
  log_level: "WARNING"
  cors:
    allow_origins: ["http://localhost:8090"]  # Only allow nginx

citation:
  chunk:
    size: 500
    overlap: 50

# Chunking Configuration (for new ChunkingService)
chunking:
  chunk_size_words: 100  # Words per chunk
  chunk_overlap_words: 20  # Overlap between chunks for context preservation
  min_chunk_size_words: 60  # Don't chunk if text is smaller than this
  chunking_mode: "eager"  # "eager" = process on create/update, "lazy" = process on first use

# Search Configuration (for new SearchService)
search:
  top_k_chunks: 5  # Number of top similar chunks to return per query
  faiss_index_type: "IndexFlatL2"  # Exact search for small-medium datasets
  min_relevance_threshold: 0.7  # Minimum cosine similarity score to consider relevant

# Referencing/Validation Configuration (for new ReferencingService)
referencing:
  max_references_per_fact: 3  # Maximum number of chunk references to keep per fact
  min_validation_confidence: 0.8  # Minimum confidence score for AI validation
  discard_if_no_references: true  # Discard facts with no valid references
